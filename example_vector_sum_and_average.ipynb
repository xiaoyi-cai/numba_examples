{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbNPCNsSRrtu"
      },
      "source": [
        "## Example of using Numba for adding two vectors and computing average of elements in a vector.\n",
        "\n",
        "Make sure to change the run time to have GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRpA59tQP-u0",
        "outputId": "f705f708-47c4-42ab-8ec6-f45a50ccf779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name = b'NVIDIA GeForce RTX 3070 Ti Laptop GPU'\n",
            "maxThreadsPerBlock = 1024\n",
            "maxBlockDimX = 1024\n",
            "maxBlockDimY = 1024\n",
            "maxBlockDimZ = 64\n",
            "maxGridDimX = 2147483647\n",
            "maxGridDimY = 65535\n",
            "maxGridDimZ = 65535\n",
            "maxSharedMemoryPerBlock = 49152\n"
          ]
        }
      ],
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gpu = cuda.get_current_device()\n",
        "print(\"name = %s\" % gpu.name)\n",
        "print(\"maxThreadsPerBlock = %s\" % str(gpu.MAX_THREADS_PER_BLOCK))\n",
        "print(\"maxBlockDimX = %s\" % str(gpu.MAX_BLOCK_DIM_X))\n",
        "print(\"maxBlockDimY = %s\" % str(gpu.MAX_BLOCK_DIM_Y))\n",
        "print(\"maxBlockDimZ = %s\" % str(gpu.MAX_BLOCK_DIM_Z))\n",
        "print(\"maxGridDimX = %s\" % str(gpu.MAX_GRID_DIM_X))\n",
        "print(\"maxGridDimY = %s\" % str(gpu.MAX_GRID_DIM_Y))\n",
        "print(\"maxGridDimZ = %s\" % str(gpu.MAX_GRID_DIM_Z))\n",
        "print(\"maxSharedMemoryPerBlock = %s\" % str(gpu.MAX_SHARED_MEMORY_PER_BLOCK))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiD1WgKAP-u2"
      },
      "source": [
        "## Vector summation\n",
        "This can be done without synchronization across blocks and threads. When the kernel is called for the first time, it takes sometime to compile the function to run on GPU, but subsequent calls are faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uI_T4hmGP-u3"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def add_numba(a, b, c):\n",
        "    # Obtain the thread id w.r.t. the grid\n",
        "    # Alternatively: threadIdx.x + (blockIdx.x * blockDim.x)\n",
        "    tid = cuda.grid(1) \n",
        "    size = len(c)\n",
        "\n",
        "    if tid < size:\n",
        "        c[tid] = a[tid] + b[tid]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdp6KDEWkCru"
      },
      "source": [
        "* The kernel is invoked by also specifying the grid and block dimensions in a square bracket. \n",
        "* Kernels do not have return values, so variables have to be passed in to store the results.\n",
        "* By default, the variables are copied to and from device automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swMTkoTIkBJs",
        "outputId": "6207c51c-5b9e-4fc1-b830-5907ecd31842"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jeremy/.local/lib/python3.8/site-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 2 0]+[0 1 2]=[0 3 2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jeremy/.local/lib/python3.8/site-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ],
      "source": [
        "a = np.random.randint(low=0, high=3, size=3)\n",
        "b = np.random.randint(low=0, high=3, size=3)\n",
        "c = np.zeros_like(a)\n",
        "num_blocks = 1\n",
        "num_threads = len(a)\n",
        "assert num_threads<gpu.MAX_THREADS_PER_BLOCK\n",
        "\n",
        "add_numba[num_blocks, num_threads](a, b, c)\n",
        "print(\"{}+{}={}\".format(a, b, c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slt_UHY5kL8X"
      },
      "source": [
        "Memory can be pre-allocated on device and copied back to host."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yi2P8o79P-u3"
      },
      "outputs": [],
      "source": [
        "# Specify a and b in host memory\n",
        "N = int(1e8)\n",
        "a_cpu = np.random.random(N)\n",
        "b_cpu = np.random.random(N)\n",
        "\n",
        "# Move a, b to device, and allocate memory for c on device\n",
        "a_gpu = cuda.to_device(np.random.random(N))\n",
        "b_gpu = cuda.to_device(np.random.random(N))\n",
        "c_gpu = cuda.device_array_like(a_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsddZ_sZP-u4",
        "outputId": "4a3499db-1ddc-4235-cf7f-3937fdef6e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First run took 0.04236555099487305s to compute on GPU.\n",
            "Second run took 0.000324249267578125s to compute on GPU.\n",
            "Took 0.1907796859741211s to copy from GPU to CPU.\n",
            "Took 0.16444754600524902s to compute on CPU.\n"
          ]
        }
      ],
      "source": [
        "# Specify the number of threads and blocks\n",
        "num_threads_per_block=100\n",
        "num_blocks = int(N//num_threads_per_block)+1\n",
        "\n",
        "# Time GPU code\n",
        "t0 = time.time()\n",
        "add_numba[num_blocks, num_threads_per_block](a_gpu, b_gpu, c_gpu)\n",
        "print(\"First run took {}s to compute on GPU.\".format(time.time()-t0))\n",
        "t0 = time.time()\n",
        "add_numba[num_blocks, num_threads_per_block](a_gpu, b_gpu, c_gpu)\n",
        "print(\"Second run took {}s to compute on GPU.\".format(time.time()-t0))\n",
        "\n",
        "t0 = time.time()\n",
        "_ = c_gpu.copy_to_host()\n",
        "print(\"Took {}s to copy from GPU to CPU.\".format(time.time()-t0))\n",
        "\n",
        "# Time CPU code\n",
        "t0 = time.time()\n",
        "c = a_cpu+b_cpu\n",
        "print(\"Took {}s to compute on CPU.\".format(time.time()-t0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLl1xfJGP-u4"
      },
      "source": [
        "## Compute sum. Need synchronization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ow1eNt0nP-u4"
      },
      "outputs": [],
      "source": [
        "@cuda.jit(fastmath=True)\n",
        "def mean_numba(v):\n",
        "    tid = cuda.threadIdx.x\n",
        "    numel = len(v)\n",
        "    num_threads = cuda.blockDim.x\n",
        "    repeat = int(math.ceil(numel/num_threads))\n",
        "\n",
        "    if tid<numel:\n",
        "        s = 1\n",
        "        while s < numel:\n",
        "            for ri in range(repeat):\n",
        "                gi = tid + ri*num_threads\n",
        "                if (gi % (2*s) == 0) and ((gi+s)<numel):\n",
        "                    v[gi] += v[gi+s]\n",
        "            s *= 2\n",
        "            cuda.syncthreads()\n",
        "        \n",
        "    if tid==0:\n",
        "        v[0] = v[0]/numel\n",
        "    \n",
        "\n",
        "v_cpu = c_gpu.copy_to_host()\n",
        "v_gpu = cuda.to_device(v_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8z6z7m-P-u5",
        "outputId": "3d65a4ce-2d38-40d4-bf0f-f766b8ec7f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First run took 0.07258892059326172s on GPU\n",
            "Second run took 0.00034928321838378906s on GPU\n",
            "0.0423429012298584s on CPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jeremy/.local/lib/python3.8/site-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "num_threads_per_block = 100\n",
        "mean_numba[1,num_threads_per_block](c_gpu)\n",
        "print(\"First run took {}s on GPU\".format(time.time()-t0))\n",
        "t0 = time.time()\n",
        "num_threads_per_block = 100\n",
        "mean_numba[1,num_threads_per_block](c_gpu)\n",
        "print(\"Second run took {}s on GPU\".format(time.time()-t0))\n",
        "\n",
        "t0 = time.time()\n",
        "mean_cpu = np.mean(v_cpu)\n",
        "print(\"{}s on CPU\".format(time.time()-t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0pGbzEhP-u5",
        "outputId": "64c3f084-4010-49a8-a04b-13da13c99cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The slowest run took 5.72 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "38.9 µs ± 33 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
            "41.3 ms ± 599 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit -n 5 -r 5 mean_numba[1,num_threads_per_block](v_gpu)\n",
        "\n",
        "%timeit -n 5 -r 5 np.mean(v_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODRaz68bQXxa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
